{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "image_model/VGG_ILSVRC_19_layers_deploy.prototxt\timage_model/VGG_ILSVRC_19_layers.caffemodel\tcudnn\t\n"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Successfully loaded image_model/VGG_ILSVRC_19_layers.caffemodel\n"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "conv1_1: 64 3 3 3\n",
       "conv1_2: 64 64 3 3\n"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "conv2_1: 128 64 3 3\n",
       "conv2_2: 128 128 3 3\n",
       "conv3_1: 256 128 3 3\n"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "conv3_2: 256 256 3 3\n"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "conv3_3: 256 256 3 3\n"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "conv3_4: 256 256 3 3\n",
       "conv4_1: 512 256 3 3\n"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "conv4_2: 512 512 3 3\n"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "conv4_3: 512 512 3 3\n"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "conv4_4: 512 512 3 3\n"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "conv5_1: 512 512 3 3\n"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "conv5_2: 512 512 3 3\n"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "conv5_3: 512 512 3 3\n"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "conv5_4: 512 512 3 3\n"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "fc6: 1 1 25088 4096\n"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "fc7: 1 1 4096 4096\n"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "fc8: 1 1 4096 1000\n"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.01 *\n",
       "-4.1915\n",
       "[torch.CudaTensor of size 1]\n",
       "\n",
       "Load the weight...\t\n"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       " 0.1113\n",
       "[torch.CudaTensor of size 1]\n",
       "\n",
       "total number of parameters in cnn_model: \t20024384\t\n",
       "total number of parameters in word_level: \t8031747\t\n",
       "total number of parameters in phrase_level: \t2889219\t\n",
       "total number of parameters in ques_level: \t5517315\t\n",
       "constructing clones inside the ques_level\t\n"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "total number of parameters in recursive_attention: \t2862056\t\n"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "require 'nn'\n",
    "require 'torch'\n",
    "require 'optim'\n",
    "require 'misc.DataLoader'\n",
    "require 'misc.word_level'\n",
    "require 'misc.phrase_level'\n",
    "require 'misc.ques_level'\n",
    "require 'misc.recursive_atten'\n",
    "require 'misc.cnnModel'\n",
    "require 'misc.optim_updates'\n",
    "utils = require 'misc.utils'\n",
    "require 'xlua'\n",
    "require 'image'\n",
    "\n",
    "\n",
    "opt = {}\n",
    "\n",
    "opt.vqa_model = 'model/vqa_model/model_alternating_train_vgg.t7'\n",
    "opt.cnn_proto = 'image_model/VGG_ILSVRC_19_layers_deploy.prototxt'\n",
    "opt.cnn_model = 'image_model/VGG_ILSVRC_19_layers.caffemodel'\n",
    "opt.json_file = 'data/vqa_data_prepro.json'\n",
    "opt.backend = 'cudnn'\n",
    "opt.gpuid = 1\n",
    "if opt.gpuid >= 0 then\n",
    "  require 'cutorch'\n",
    "  require 'cunn'\n",
    "  if opt.backend == 'cudnn' then \n",
    "  require 'cudnn' \n",
    "  end\n",
    "  --cutorch.setDevice(opt.gpuid+1) -- note +1 because lua is 1-indexed\n",
    "end\n",
    "\n",
    "local loaded_checkpoint = torch.load(opt.vqa_model)\n",
    "local lmOpt = loaded_checkpoint.lmOpt\n",
    "\n",
    "lmOpt.hidden_size = 512\n",
    "lmOpt.feature_type = 'VGG'\n",
    "lmOpt.atten_type = 'Alternating'\n",
    "cnnOpt = {}\n",
    "cnnOpt.cnn_proto = opt.cnn_proto\n",
    "cnnOpt.cnn_model = opt.cnn_model\n",
    "cnnOpt.backend = opt.backend\n",
    "cnnOpt.input_size_image = 512\n",
    "cnnOpt.output_size = 512\n",
    "cnnOpt.h = 14\n",
    "cnnOpt.w = 14\n",
    "cnnOpt.layer_num = 37\n",
    "\n",
    "-- load the vocabulary and answers.\n",
    "\n",
    "local json_file = utils.read_json(opt.json_file)\n",
    "ix_to_word = json_file.ix_to_word\n",
    "ix_to_ans = json_file.ix_to_ans\n",
    "\n",
    "word_to_ix = {}\n",
    "for ix, word in pairs(ix_to_word) do\n",
    "    word_to_ix[word]=ix\n",
    "end\n",
    "\n",
    "-- load the model\n",
    "protos = {}\n",
    "protos.word = nn.word_level(lmOpt)\n",
    "protos.phrase = nn.phrase_level(lmOpt)\n",
    "protos.ques = nn.ques_level(lmOpt)\n",
    "\n",
    "protos.atten = nn.recursive_atten()\n",
    "protos.crit = nn.CrossEntropyCriterion()\n",
    "protos.cnn = nn.cnnModel(cnnOpt)\n",
    "\n",
    "if opt.gpuid >= 0 then\n",
    "  for k,v in pairs(protos) do v:cuda() end\n",
    "end\n",
    "\n",
    "cparams, grad_cparams = protos.cnn:getParameters()\n",
    "wparams, grad_wparams = protos.word:getParameters()\n",
    "pparams, grad_pparams = protos.phrase:getParameters()\n",
    "qparams, grad_qparams = protos.ques:getParameters()\n",
    "aparams, grad_aparams = protos.atten:getParameters()\n",
    "\n",
    "print(wparams:sub(1,1))\n",
    "print('Load the weight...')\n",
    "wparams:copy(loaded_checkpoint.wparams)\n",
    "pparams:copy(loaded_checkpoint.pparams)\n",
    "qparams:copy(loaded_checkpoint.qparams)\n",
    "aparams:copy(loaded_checkpoint.aparams)\n",
    "print(pparams:sub(1,10))\n",
    "\n",
    "print('total number of parameters in cnn_model: ', cparams:nElement())\n",
    "assert(cparams:nElement() == grad_cparams:nElement())\n",
    "\n",
    "print('total number of parameters in word_level: ', wparams:nElement())\n",
    "assert(wparams:nElement() == grad_wparams:nElement())\n",
    "\n",
    "print('total number of parameters in phrase_level: ', pparams:nElement())\n",
    "assert(pparams:nElement() == grad_pparams:nElement())\n",
    "\n",
    "print('total number of parameters in ques_level: ', qparams:nElement())\n",
    "assert(qparams:nElement() == grad_qparams:nElement())\n",
    "protos.ques:shareClones()\n",
    "\n",
    "print('total number of parameters in recursive_attention: ', aparams:nElement())\n",
    "assert(aparams:nElement() == grad_aparams:nElement())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "[string \"-- specify the image and the question....\"]:46: ')' expected near 'ans'",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "[string \"-- specify the image and the question....\"]:46: ')' expected near 'ans'"
     ]
    }
   ],
   "source": [
    "-- specify the image and the question.\n",
    "local img_path = 'visu/demo_img1.jpg'\n",
    "local question = 'what is the color of the hat ?'\n",
    "\n",
    "-- load the image\n",
    "local img = image.load(img_path)\n",
    "-- scale the image\n",
    "img = image.scale(img,448,448)\n",
    "itorch.image(img)\n",
    "img = img:view(1,img:size(1),img:size(2),img:size(3))\n",
    "-- parse and encode the question (in a simple way).\n",
    "local ques_encode = torch.IntTensor(26):zero()\n",
    "\n",
    "local count = 1\n",
    "for word in string.gmatch(question, \"%S+\") do\n",
    "    ques_encode[count] = word_to_ix[word] or word_to_ix['UNK']\n",
    "    count = count + 1\n",
    "end\n",
    "ques_encode = ques_encode:view(1,ques_encode:size(1))\n",
    "-- doing the prediction\n",
    "\n",
    "protos.word:evaluate()\n",
    "protos.phrase:evaluate()\n",
    "protos.ques:evaluate()\n",
    "protos.atten:evaluate()\n",
    "protos.cnn:evaluate()\n",
    "\n",
    "local image_raw = utils.prepro(img, false)\n",
    "image_raw = image_raw:cuda()\n",
    "ques_encode = ques_encode:cuda()\n",
    "\n",
    "local image_feat = protos.cnn:forward(image_raw)\n",
    "local ques_len = torch.Tensor(1,1):cuda()\n",
    "ques_len[1] = count-1\n",
    "\n",
    "local word_feat, img_feat, w_ques, w_img, mask = unpack(protos.word:forward({ques_encode, image_feat}))\n",
    "local conv_feat, p_ques, p_img = unpack(protos.phrase:forward({word_feat, ques_len, img_feat, mask}))\n",
    "local q_ques, q_img = unpack(protos.ques:forward({conv_feat, ques_len, img_feat, mask}))\n",
    "\n",
    "local feature_ensemble = {w_ques, w_img, p_ques, p_img, q_ques, q_img}\n",
    "local out_feat = protos.atten:forward(feature_ensemble)\n",
    "\n",
    "local tmp,pred=torch.max(out_feat,2)\n",
    "local ans = ix_to_ans[tostring(pred[1][1])]\n",
    "\n",
    "print('The answer is:' ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-- Attention Visualization\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iTorch",
   "language": "lua",
   "name": "itorch"
  },
  "language_info": {
   "name": "lua",
   "version": "5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
